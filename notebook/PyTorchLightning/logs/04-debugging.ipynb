{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debuggging PyTorch Lightning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from typing import Any, Optional, Sequence, TypeAlias, Union\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"info\": \"#76FF7B\",\n",
    "        \"warning\": \"#FBDDFE\",\n",
    "        \"error\": \"#FF0000\",\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_up_from_current_directory(go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.8\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "polars      : 0.20.18\n",
      "scikit-learn: 1.4.1.post1\n",
      "torch       : 2.2.2\n",
      "lightning   : 2.2.1\n",
      "\n",
      "Torch CUDA available?:  False\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from watermark import watermark\n",
    "\n",
    "Model: TypeAlias = nn.Module\n",
    "\n",
    "print(watermark(packages=\"polars,scikit-learn,torch,lightning\", python=True))\n",
    "print(\"Torch CUDA available?: \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lightning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(L.LightningModule):\n",
    "    def __init__(self, model: nn.Module, learning_rate: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.test_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def _shared_steps(self, batch) -> torch.Tensor:\n",
    "        # Fetch the data\n",
    "        features, true_labels = batch\n",
    "\n",
    "        # Forward prop\n",
    "        logits: torch.Tensor = self(features)\n",
    "        predited_labels: torch.Tensor = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss: torch.Tensor = F.cross_entropy(logits, true_labels)\n",
    "\n",
    "        return loss, true_labels, predited_labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        # Shared steps\n",
    "        loss, true_labels, predited_labels = self._shared_steps(batch=batch)\n",
    "\n",
    "        # Compute accuracy\n",
    "        self.train_accuracy(true_labels, predited_labels)\n",
    "\n",
    "        # Log loss and accuracy\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\n",
    "            \"train_accuracy\",\n",
    "            self.train_accuracy,\n",
    "            prog_bar=True,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        # Shared steps\n",
    "        loss, true_labels, predited_labels = self._shared_steps(batch=batch)\n",
    "\n",
    "        # Compute accuracy\n",
    "        self.val_accuracy(true_labels, predited_labels)\n",
    "\n",
    "        # Log loss and accuracy\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\n",
    "            \"val_accuracy\",\n",
    "            self.val_accuracy,\n",
    "            prog_bar=True,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx) -> torch.Tensor:\n",
    "        # Shared steps\n",
    "        loss, true_labels, predited_labels = self._shared_steps(batch=batch)\n",
    "\n",
    "        # Compute accuracy\n",
    "        self.test_accuracy(true_labels, predited_labels)\n",
    "\n",
    "        # Log accuracy\n",
    "        self.log(\"test_accuracy\", self.test_accuracy)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> dict[str, Any]:\n",
    "        optimizer: Any = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler: Any = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", factor=0.1, patience=5\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "                \"intervals\": \"epoch\",  # default\n",
    "                \"frequency\": 1,  # default\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "NDArray: TypeAlias = npt.NDArray[np.float_ | np.int_]\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, features: torch.Tensor, labels: torch.Tensor, transform: Any | None = None\n",
    "    ) -> None:\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return feature, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lightning Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class LitDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = \"../../data/classification_data.csv\",\n",
    "        batch_size: int = 32,\n",
    "        seed: int = 123,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"This optional method is used to download the data.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        # Create data\n",
    "        X, y = make_classification(\n",
    "            n_samples=20_000,\n",
    "            n_classes=2,\n",
    "            n_features=100,\n",
    "            n_informative=10,\n",
    "            n_redundant=40,\n",
    "            n_repeated=25,\n",
    "            n_clusters_per_class=5,\n",
    "            flip_y=0.05,\n",
    "            class_sep=0.5,\n",
    "            random_state=123,\n",
    "        )\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=self.seed, stratify=y\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=0.1, random_state=self.seed, stratify=y_train\n",
    "        )\n",
    "        self.train_dataset = CustomDataset(features=X_train, labels=y_train)\n",
    "        self.val_dataset = CustomDataset(features=X_val, labels=y_val)\n",
    "        self.test_dataset = CustomDataset(features=X_test, labels=y_test)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=True,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.val_dataset, batch_size=self.batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.test_dataset, batch_size=self.batch_size, shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_features: int, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.all_layers = nn.Sequential(\n",
    "            # 1st hidden layer\n",
    "            nn.Linear(num_features, 64),\n",
    "            nn.ReLU(),\n",
    "            # 2nd hidden layer\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            # Output layer\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x = torch.flatten(input=x, start_dim=1)\n",
    "        logits: torch.Tensor = self.all_layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `fast_dev_run` To Limit The Number of Iterations\n",
    "\n",
    "- This is very useful for debugging your code before you train a model for a long time.\n",
    "\n",
    "```py\n",
    "# Trainer\n",
    "trainer = L.Trainer(\n",
    "    fast_dev_run=5,  # Set the number of epochs to N for a quick test\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    max_epochs=20, \n",
    "    deterministic=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 5 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | model          | MLP                | 8.6 K \n",
      "1 | train_accuracy | MulticlassAccuracy | 0     \n",
      "2 | val_accuracy   | MulticlassAccuracy | 0     \n",
      "3 | test_accuracy  | MulticlassAccuracy | 0     \n",
      "------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c3cf17a00144c1aed8a8557f713ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfa82850d504d36b16b5747b2413cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5` reached.\n"
     ]
    }
   ],
   "source": [
    "# Set seed\n",
    "L.seed_everything(123)\n",
    "\n",
    "data_module: LitDataModule = LitDataModule()\n",
    "\n",
    "# Lightning Model\n",
    "pytorch_model = MLP(num_features=100, num_classes=2)\n",
    "lightning_model: LitModel = LitModel(model=pytorch_model, learning_rate=0.05)\n",
    "\n",
    "# Trainer\n",
    "trainer = L.Trainer(\n",
    "    fast_dev_run=5,  # Set the number of epochs to N for a quick test\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    max_epochs=20,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "# Fit\n",
    "trainer.fit(model=lightning_model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  | Name               | Type               | Params\n",
      "----------------------------------------------------------\n",
      "0 | model              | MLP                | 8.6 K \n",
      "1 | model.all_layers   | Sequential         | 8.6 K \n",
      "2 | model.all_layers.0 | Linear             | 6.5 K \n",
      "3 | model.all_layers.1 | ReLU               | 0     \n",
      "4 | model.all_layers.2 | Linear             | 2.1 K \n",
      "5 | model.all_layers.3 | ReLU               | 0     \n",
      "6 | model.all_layers.4 | Linear             | 66    \n",
      "7 | train_accuracy     | MulticlassAccuracy | 0     \n",
      "8 | val_accuracy       | MulticlassAccuracy | 0     \n",
      "9 | test_accuracy      | MulticlassAccuracy | 0     \n",
      "----------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "\n",
    "\n",
    "summary = ModelSummary(lightning_model, max_depth=-1)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
