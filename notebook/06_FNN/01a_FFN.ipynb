{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Networks (FFN) \n",
    "\n",
    "- AKA Artificial Neural Networks (ANN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import logging\n",
    "from typing import Any, Optional, Sequence, Union\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure the backend\n",
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 2_000\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n",
    "import seaborn as sns\n",
    "\n",
    "# Custom import\n",
    "from src.utilities import (\n",
    "    set_up_logger,\n",
    "    create_iris_data,\n",
    "    create_qwerties_data,\n",
    "    smooth,\n",
    ")\n",
    "from src.data_manager import (\n",
    "    load_data,\n",
    "    create_data_loader,\n",
    "    split_into_train_n_validation,\n",
    ")\n",
    "from src.preprocessor import Standardizer, Normalizer\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load/Download MNIST Digits Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"../../data/mnist_digit\"\n",
    "\n",
    "# Chain multiple transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # Convert to PyTorch tensors\n",
    "        transforms.ToTensor(),\n",
    "        #  It performs per-channel normalization, where each channel\n",
    "        # (e.g., red, green, blue for an RGB image) is normalized independently.\n",
    "        # Since it's a single channel, we have (0.5,)\n",
    "        transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = MNIST(root=fp, train=True, transform=transform, download=True)\n",
    "test_dataset = MNIST(root=fp, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ../../data/mnist_digit\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "RANDOM_STATE = 123\n",
    "\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on device=cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# If we're on a CUDA machine, this should print a CUDA device:\n",
    "print(f\"Working on device={device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Each MNSIT image is 1x28x28, so it is an 2D array [28,28]\n",
    "# I'll flatten the image as vector dim=1*28*28\n",
    "input_size = 1 * 28 * 28\n",
    "hidden_size = 128\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))  # 80% of training data\n",
    "val_size = len(train_dataset) - train_size  # 20% of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset=train_dataset,\n",
    "    lengths=[train_size, val_size],\n",
    ")\n",
    "\n",
    "# Create DataLoader for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"This is used to build a Feed Forward Network architecture that\n",
    "    is used for classification.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"This performs the forward propagation.\"\"\"\n",
    "        # Flatten the input images\n",
    "        x = x.view(-1, (28 * 28))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0669, -0.0147, -0.0402,  ..., -0.0764,  0.0500,  0.0240],\n",
       "        [-0.1014,  0.0012, -0.0063,  ..., -0.0860,  0.0868,  0.0196],\n",
       "        [-0.1144, -0.0045, -0.0631,  ..., -0.0724,  0.0064, -0.0021],\n",
       "        ...,\n",
       "        [-0.0906, -0.0359, -0.0019,  ..., -0.0985,  0.0290,  0.0308],\n",
       "        [-0.0753, -0.0220, -0.0032,  ..., -0.0829,  0.0367,  0.0334],\n",
       "        [-0.0603, -0.0191, -0.0079,  ..., -0.1157,  0.0658,  0.0115]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model with random data\n",
    "ffn = FFN(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n",
    "x_ = torch.rand(size=(1_000, 28, 28))\n",
    "x_.shape\n",
    "result = ffn.forward(x=x_)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0914, 0.0868, 0.0698, 0.1125, 0.1337], grad_fn=<SliceBackward0>),\n",
       " tensor([5, 8, 3, 8, 3]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It returns the value and the index.\n",
    "# We're interested in the index\n",
    "values, _labels = torch.max(result, dim=1)\n",
    "values[:5], _labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 8, 3, 8, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OR\n",
    "_labels = torch.argmax(result, dim=1)\n",
    "_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836a97bdae404c78af8e77d9ca6ffdae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Training Loss: 0.4448000779549281, Validation Loss: 0.0010753319935595735, Validation Accuracy: 91.18%\n",
      "Epoch 2/5, Training Loss: 0.21855246077477933, Validation Loss: 0.0002921261685959836, Validation Accuracy: 94.29%\n",
      "Epoch 3/5, Training Loss: 0.15856496324390174, Validation Loss: 0.00029735376146879604, Validation Accuracy: 95.35%\n",
      "Epoch 4/5, Training Loss: 0.12930977767209212, Validation Loss: 0.00021179367173859413, Validation Accuracy: 96.05%\n",
      "Epoch 5/5, Training Loss: 0.11113543171621859, Validation Loss: 0.00017865598598059188, Validation Accuracy: 95.94%\n",
      "Test Accuracy: 96.12%\n"
     ]
    }
   ],
   "source": [
    "# ==== Init model ====\n",
    "model = FFN(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_classes=num_classes,\n",
    ").to(device=device)\n",
    "\n",
    "# ==== Define loss function and optimizer ====\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ==== Training loop ====\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # ==== Batch training loop ====\n",
    "    for images, labels in train_loader:\n",
    "        # Push the data to GPU if available\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # ==== Forwardprop ====\n",
    "        outputs = model(images)\n",
    "        loss: nn.CrossEntropyLoss = criterion(outputs, labels)\n",
    "\n",
    "        # ==== Backprop ====\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # ==== Validation loop ====\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            # Push the data to GPU if available\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss = criterion(outputs, labels).item()\n",
    "            # It returns the value and the index.\n",
    "            # We're interested in the index\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            val_total += labels.size(0)  # or labels.shape[0]\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = (val_correct / val_total) * 100\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "        f\"Training Loss: {running_loss / len(train_loader)}, \"\n",
    "        f\"Validation Loss: {val_loss / len(val_loader)}, \"\n",
    "        f\"Validation Accuracy: {val_accuracy:.2f}%\"\n",
    "    )\n",
    "\n",
    "# Test the model on the test dataset\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # Push the data to GPU if available\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # It returns the value and the index.\n",
    "        # We're interested in the index\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = (test_correct / test_total) * 100\n",
    "print(f\"Test Accuracy: { test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"This is used to build a Feed Forward Network architecture that\n",
    "    is used for classification.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"This performs the forward propagation.\"\"\"\n",
    "        # Flatten the input images\n",
    "        x = x.view(-1, (28 * 28))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: FFN,\n",
    "    device: Any,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: Any,\n",
    "    optimizer: torch.optim,\n",
    "    num_epochs: int,\n",
    ") -> FFN:\n",
    "    \"\"\"This is used for training the model.\"\"\"\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # ==== Batch training loop ====\n",
    "        for images, labels in train_loader:\n",
    "            # Push the data to GPU if available\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # ==== Forwardprop ====\n",
    "            outputs = model(images)\n",
    "            loss: nn.CrossEntropyLoss = criterion(outputs, labels)\n",
    "\n",
    "            # ==== Backprop ====\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Update the loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # ==== Validation loop ====\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                # Push the data to GPU if available\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                val_loss = criterion(outputs, labels).item()\n",
    "                # It returns the value and the index.\n",
    "                # We're interested in the index\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                val_total += labels.size(0)  # or labels.shape[0]\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        val_accuracy = (val_correct / val_total) * 100\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "            f\"Training Loss: {running_loss / len(train_loader)}, \"\n",
    "            f\"Validation Loss: {val_loss / len(val_loader)}, \"\n",
    "            f\"Validation Accuracy: {val_accuracy:.2f}%\"\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(model: FFN, device: Any, test_loader: DataLoader):\n",
    "    \"\"\"This is used to the model on the test dataset.\"\"\"\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # Push the data to GPU if available\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = (test_correct / test_total) * 100\n",
    "    print(f\"Test Accuracy: { test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper-parameters\n",
    "\n",
    "# Each MNSIT image is 1x28x28, so it is an 2D array [28,28]\n",
    "# I'll flatten the image as vector dim=1*28*28\n",
    "input_size = 1 * 28 * 28\n",
    "hidden_size = 128\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))  # 80% of training data\n",
    "val_size = len(train_dataset) - train_size  # 20% of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"This is the main function.\"\"\"\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # If we're on a CUDA machine, this should print a CUDA device:\n",
    "    print(f\"Working on device={device!r}\")\n",
    "\n",
    "    # ==== Init model ====\n",
    "    model = FFN(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_classes=num_classes,\n",
    "    ).to(device=device)\n",
    "\n",
    "    # ==== Define loss function and optimizer ====\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # ==== Train the model ====\n",
    "    model = train(\n",
    "        model,\n",
    "        device=device,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    # ==== Evaluate the model ====\n",
    "    test(model, device=device, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on device=device(type='cpu')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ee0ee9129f4d2a96071e107a3d5de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.42105956345796586, Validation Loss: 0.0008805161143871064, Validation Accuracy: 92.10%\n",
      "Epoch 2/10, Training Loss: 0.2069486830085516, Validation Loss: 0.00026071828572039906, Validation Accuracy: 94.63%\n",
      "Epoch 3/10, Training Loss: 0.15061235540608564, Validation Loss: 0.0002729851316581381, Validation Accuracy: 95.68%\n",
      "Epoch 4/10, Training Loss: 0.11916445186485847, Validation Loss: 0.00011550384434930821, Validation Accuracy: 95.88%\n",
      "Epoch 5/10, Training Loss: 0.10217102098837495, Validation Loss: 0.00022950998329101724, Validation Accuracy: 96.29%\n",
      "Epoch 6/10, Training Loss: 0.08659990383436282, Validation Loss: 0.00013341131481401465, Validation Accuracy: 96.64%\n",
      "Epoch 7/10, Training Loss: 0.07828768156965574, Validation Loss: 2.9218328641133106e-05, Validation Accuracy: 96.78%\n",
      "Epoch 8/10, Training Loss: 0.06601344964032371, Validation Loss: 0.00018226797197093354, Validation Accuracy: 95.30%\n",
      "Epoch 9/10, Training Loss: 0.0628584173200652, Validation Loss: 3.299560705001684e-05, Validation Accuracy: 96.92%\n",
      "Epoch 10/10, Training Loss: 0.05685800845346724, Validation Loss: 0.00010487360959040358, Validation Accuracy: 97.22%\n",
      "Test Accuracy: 96.97%\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
